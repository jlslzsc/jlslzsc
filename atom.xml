<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>冷眸</title>
  
  <subtitle>丞</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jlslzsc.github.io/"/>
  <updated>2021-01-20T16:08:46.000Z</updated>
  <id>http://jlslzsc.github.io/</id>
  
  <author>
    <name>Zhou ShuCheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hadoop集群搭建</title>
    <link href="http://jlslzsc.github.io/2021/01/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://jlslzsc.github.io/2021/01/20/Hadoop集群搭建/</id>
    <published>2021-01-20T15:32:32.000Z</published>
    <updated>2021-01-20T16:08:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、xsync脚本文件"><a href="#1、xsync脚本文件" class="headerlink" title="1、xsync脚本文件"></a>1、xsync脚本文件</h2><p>在/usr/local/bin 目录下创建 xsync 文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line"><span class="meta">#</span>1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>2 获取文件名称</span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>3 获取上级目录到绝对路径</span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>4 获取当前用户名称</span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>5 循环</span><br><span class="line">for((host=102; host&lt;104;host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="2、ssh无密登录"><a href="#2、ssh无密登录" class="headerlink" title="2、ssh无密登录"></a>2、ssh无密登录</h2><p>（1）生成公钥和私钥：</p><p>ssh-keygen -t rsa</p><p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p><p>（2）将公钥拷贝到要免密登录的目标机器上</p><p>ssh-copy-id hadoop102</p><p>ssh-copy-id hadoop103</p><p>ssh-copy-id hadoop104</p><p>然后xsync .ssh同步到三台服务器上</p><h2 id="3、集群配置"><a href="#3、集群配置" class="headerlink" title="3、集群配置"></a>3、集群配置</h2><ol><li>集群部署规划</li></ol><p>​    注意：NameNode和SecondaryNameNode不要安装在同一台服务器</p><p>​    注意：ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</p><p>表2-3</p><table><thead><tr><th></th><th>hadoop101</th><th>hadoop102</th><th>hadoop103</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNodeDataNode</td><td>DataNode</td><td>SecondaryNameNode</td></tr><tr><td></td><td></td><td></td><td>DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager</td><td>NodeManager</td></tr><tr><td></td><td></td><td>NodeManager</td><td></td></tr></tbody></table><ol start="2"><li>配置集群</li></ol><p>​    （1）核心配置文件</p><p>配置core-site.xml</p><p>cd $HADOOP_HOME/etc/hadoop</p><p>vim core-site.xml</p><p>文件内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>​    （2）HDFS配置文件</p><p>配置hdfs-site.xml</p><p>vim hdfs-site.xml</p><p>文件内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/namesecondary<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.datanode-restart.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>30<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（3）YARN配置文件</p><p>配置yarn-site.xml</p><p>vim yarn-site.xml</p><p>文件内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）MapReduce配置文件</p><p>配置mapred-site.xml</p><p>vim mapred-site.xml</p><p>文件内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>(5)workers</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop101</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br></pre></td></tr></table></figure><h2 id="4、集群单点启动"><a href="#4、集群单点启动" class="headerlink" title="4、集群单点启动"></a>4、集群单点启动</h2><p>（1）如果集群是第一次启动，需要格式化NameNode</p><p>hdfs namenode -format</p><p>（2）在hadoop102上启动NameNode</p><p>hdfs –daemon start namenode</p><p>完成后执行jps命令，看到如下结果（进程号可能不同）：</p><p>3461 NameNode</p><p>（3）在hadoop102、hadoop103以及hadoop104上执行如下命令（三台都要执行）</p><p>hdfs –daemon start datanode</p><h2 id="5、配置历史服务器"><a href="#5、配置历史服务器" class="headerlink" title="5、配置历史服务器"></a>5、配置历史服务器</h2><h4 id="1-配置mapred-site-xml"><a href="#1-配置mapred-site-xml" class="headerlink" title="1.    配置mapred-site.xml"></a>1.    配置mapred-site.xml</h4><p>vi mapred-site.xml</p><p>在该文件里面增加如下配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="2配置日志的聚集"><a href="#2配置日志的聚集" class="headerlink" title="2配置日志的聚集"></a>2配置日志的聚集</h4><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。</p><p>日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。</p><p>注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。</p><p>开启日志聚集功能具体步骤如下：</p><ol><li>配置yarn-site.xml</li></ol><p>vim yarn-site.xml</p><p>在该文件里面增加如下配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>分发配置</p><p>xsync /etc/hadoop</p><ol start="3"><li>在hadoop102启动历史服务器</li></ol><p>mr-jobhistory-daemon.sh start historyserver</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、xsync脚本文件&quot;&gt;&lt;a href=&quot;#1、xsync脚本文件&quot; class=&quot;headerlink&quot; title=&quot;1、xsync脚本文件&quot;&gt;&lt;/a&gt;1、xsync脚本文件&lt;/h2&gt;&lt;p&gt;在/usr/local/bin 目录下创建 xsync 文件&lt;/p&gt;
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://jlslzsc.github.io/categories/Hadoop/"/>
    
    
      <category term="环境搭建" scheme="http://jlslzsc.github.io/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>多线程创建方式</title>
    <link href="http://jlslzsc.github.io/2021/01/14/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F/"/>
    <id>http://jlslzsc.github.io/2021/01/14/多线程创建方式/</id>
    <published>2021-01-14T09:02:54.000Z</published>
    <updated>2021-01-14T09:06:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-继承Thread类"><a href="#1-继承Thread类" class="headerlink" title="1 继承Thread类"></a>1 继承Thread类</h2><p>①定义子类继承Thread类。</p><p>②子类中重写Thread类中的run方法。</p><p>③创建Thread子类对象，即创建了线程对象。</p><p>④调用线程对象start方法启动线程，默认调用run方法。</p><p>注意：如果只是调用run方法，则此时会在调用该方法的线程中来执行，而不是另启动一个线程。</p><h2 id="2-实现Runnable接口"><a href="#2-实现Runnable接口" class="headerlink" title="2 实现Runnable接口"></a>2 实现Runnable接口</h2><p>①定义子类，实现Runnable接口。</p><p>②子类中重写Runnable接口中的run方法。</p><p>③通过Thread类含参构造器创建线程对象，将Runnable接口的子类对象作为实际参数传递给</p><p>   Thread类的构造方法中。</p><p>④调用Thread类的start方法启动线程，其最终调用Runnable子类接口的run方法。</p><p>两种方式的区别：</p><p>继承Thread:线程代码存放Thread子类run方法中。</p><p>实现Runnable：线程代码存在接口的子类的run方法中。</p><p>实现Runnable接口避免了单继承的局限性，多个线程可以共享同一个接口子类的对象，非常适合多个相同线程来处理同一份资源。</p><p><strong>优先使用实现接口的方式!</strong></p><h2 id="3-使用Callable接口"><a href="#3-使用Callable接口" class="headerlink" title="3 使用Callable接口"></a>3 使用Callable接口</h2><p>​    Callable接口作为JDK1.5新增的接口，与使用Runnable相比其功能更强大些。</p><p>l 相比run()方法，可以有返回值</p><p>l 方法可以抛出异常</p><p>l 支持泛型的返回值</p><p>l 需要借助FutureTask类，比如获取返回结果。</p><p>Callable接口一般用于配合ExecutorService使用。</p><p>l 可以对具体Runnable、Callable任务的执行结果进行取消、查询是否完成、获取结果等。</p><p>l FutrueTask是Futrue接口的实现类</p><p>l FutureTask 同时实现了Runnable, Future接口。它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。</p><p>l 多个线程同时执行一个FutureTask，只要一个线程执行完毕，其他线程不会再执行其call()方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">implements</span> <span class="title">Callable</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">System.out.println(Thread.currentThread().getName()+<span class="string">" Come in call"</span>);</span><br><span class="line"><span class="comment">//睡5秒</span></span><br><span class="line">TimeUnit.SECONDS.sleep(<span class="number">5</span>);</span><br><span class="line"><span class="comment">//返回200的状态码</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">200</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CallableTest</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, ExecutionException </span>&#123;</span><br><span class="line">MyThread myThread = <span class="keyword">new</span> MyThread();</span><br><span class="line">FutureTask&lt;Integer&gt; futureTask = <span class="keyword">new</span> FutureTask&lt;&gt;(myThread);</span><br><span class="line"><span class="keyword">new</span> Thread(futureTask, <span class="string">"未来任务"</span>).start();</span><br><span class="line">        System.out.println(<span class="string">"主线程结束！"</span>);</span><br><span class="line">Integer integer = futureTask.get();</span><br><span class="line">System.out.println(integer);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-继承Thread类&quot;&gt;&lt;a href=&quot;#1-继承Thread类&quot; class=&quot;headerlink&quot; title=&quot;1 继承Thread类&quot;&gt;&lt;/a&gt;1 继承Thread类&lt;/h2&gt;&lt;p&gt;①定义子类继承Thread类。&lt;/p&gt;
&lt;p&gt;②子类中重写Threa
      
    
    </summary>
    
      <category term="java" scheme="http://jlslzsc.github.io/categories/java/"/>
    
    
      <category term="多线程" scheme="http://jlslzsc.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>scrapy相关配置使用</title>
    <link href="http://jlslzsc.github.io/2019/09/22/scrapy%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8/"/>
    <id>http://jlslzsc.github.io/2019/09/22/scrapy相关配置使用/</id>
    <published>2019-09-22T07:10:36.000Z</published>
    <updated>2021-01-11T10:12:24.000Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th>item使用</th></tr></thead><tbody><tr><td><img src="//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/item%E4%BD%BF%E7%94%A8.jpg" alt="item使用"></td></tr><tr><td>item使用2</td></tr><tr><td><img src="//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/item%E4%BD%BF%E7%94%A82.jpg" alt="item使用2"></td></tr><tr><td>scrapy翻页</td></tr><tr><td><img src="//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/scrapy%E7%BF%BB%E9%A1%B5.jpg" alt="scrapy翻页"></td></tr><tr><td>scrapy post请求</td></tr><tr><td><img src="//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/1.jpg" alt="1"></td></tr><tr><td>scrapy post 请求2</td></tr><tr><td><img src="//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/2.jpg" alt="2"></td></tr><tr><td>scrapy cookies登录</td></tr><tr><td><img src="//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/3.jpg" alt="3"></td></tr><tr><td>scrapy cookies 登录1</td></tr><tr><td><img src="//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/4.jpg" alt="4"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;item使用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;//jlslzsc.github.io/2019/09/22/scrapy相关配置使用/item%E4%BD%BF%E7%9
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy" scheme="http://jlslzsc.github.io/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>scrapy与scrapy crawl分布式案例</title>
    <link href="http://jlslzsc.github.io/2019/09/22/scrapy%E4%B8%8Escrapy-crawl%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%88%E4%BE%8B/"/>
    <id>http://jlslzsc.github.io/2019/09/22/scrapy与scrapy-crawl分布式案例/</id>
    <published>2019-09-22T06:49:36.000Z</published>
    <updated>2021-01-11T10:12:32.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>dangdang.py </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy_redis.spiders import RedisSpider</span><br><span class="line">from copy import deepcopy</span><br><span class="line">import urllib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DangdangSpider(RedisSpider):</span><br><span class="line">    name = &apos;dangdang&apos;</span><br><span class="line">    allowed_domains = [&apos;dangdang.com&apos;]</span><br><span class="line">    # start_urls = [&apos;http://book.dangdang.com/&apos;]</span><br><span class="line">    redis_key = &quot;dangdang&quot;</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        #大分类分组</span><br><span class="line">        div_list = response.xpath(&quot;//div[@class=&apos;con flq_body&apos;]/div&quot;)</span><br><span class="line">        for div in div_list:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[&quot;b_cate&quot;] = div.xpath(&quot;./dl/dt//text()&quot;).extract()</span><br><span class="line">            item[&quot;b_cate&quot;] = [i.strip() for i in item[&quot;b_cate&quot;] if len(i.strip())&gt;0]</span><br><span class="line">            #中间分类分组</span><br><span class="line">            dl_list = div.xpath(&quot;./div//dl[@class=&apos;inner_dl&apos;]&quot;)</span><br><span class="line">            for dl in dl_list:</span><br><span class="line">                item[&quot;m_cate&quot;] = dl.xpath(&quot;./dt//text()&quot;).extract()</span><br><span class="line">                item[&quot;m_cate&quot;] = [i.strip() for i in item[&quot;m_cate&quot;] if len(i.strip())&gt;0][0]</span><br><span class="line">                #小分类分组</span><br><span class="line">                a_list = dl.xpath(&quot;./dd/a&quot;)</span><br><span class="line">                for a in a_list:</span><br><span class="line">                    item[&quot;s_href&quot;] = a.xpath(&quot;./@href&quot;).extract_first()</span><br><span class="line">                    item[&quot;s_cate&quot;] = a.xpath(&quot;./text()&quot;).extract_first()</span><br><span class="line">                    if item[&quot;s_href&quot;] is not None:</span><br><span class="line">                        yield scrapy.Request(</span><br><span class="line">                            item[&quot;s_href&quot;],</span><br><span class="line">                            callback=self.parse_book_list,</span><br><span class="line">                            meta = &#123;&quot;item&quot;:deepcopy(item)&#125;</span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line">    def parse_book_list(self,response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        li_list = response.xpath(&quot;//ul[@class=&apos;bigimg&apos;]/li&quot;)</span><br><span class="line">        for li in li_list:</span><br><span class="line">            item[&quot;book_img&quot;] = li.xpath(&quot;./a[@class=&apos;pic&apos;]/img/@src&quot;).extract_first()</span><br><span class="line">            if item[&quot;book_img&quot;] == &quot;images/model/guan/url_none.png&quot;:</span><br><span class="line">                item[&quot;book_img&quot;] = li.xpath(&quot;./a[@class=&apos;pic&apos;]/img/@data-original&quot;).extract_first()</span><br><span class="line">            item[&quot;book_name&quot;] = li.xpath(&quot;./p[@class=&apos;name&apos;]/a/@title&quot;).extract_first()</span><br><span class="line">            item[&quot;book_desc&quot;] = li.xpath(&quot;./p[@class=&apos;detail&apos;]/text()&quot;).extract_first()</span><br><span class="line">            item[&quot;book_price&quot;] = li.xpath(&quot;.//span[@class=&apos;search_now_price&apos;]/text()&quot;).extract_first()</span><br><span class="line">            item[&quot;book_author&quot;] = li.xpath(&quot;./p[@class=&apos;search_book_author&apos;]/span[1]/a/text()&quot;).extract()</span><br><span class="line">            item[&quot;book_publish_date&quot;] = li.xpath(&quot;./p[@class=&apos;search_book_author&apos;]/span[2]/text()&quot;).extract_first()</span><br><span class="line">            item[&quot;book_press&quot;] = li.xpath(&quot;./p[@class=&apos;search_book_author&apos;]/span[3]/a/text()&quot;).extract_first()</span><br><span class="line">            print(item)</span><br><span class="line">        #下一页</span><br><span class="line">        next_url = response.xpath(&quot;//li[@class=&apos;next&apos;]/a/@href&quot;).extract_first()</span><br><span class="line">        if next_url is not None:</span><br><span class="line">            next_url = urllib.parse.urljoin(response.url,next_url)</span><br><span class="line">            yield  scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                callback=self.parse_book_list,</span><br><span class="line">                meta = &#123;&quot;item&quot;:item&#125;</span><br><span class="line">            )</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>amazno.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">class AmazonSpider(RedisCrawlSpider):</span><br><span class="line">    name = &apos;amazon&apos;</span><br><span class="line">    allowed_domains = [&apos;amazon.cn&apos;]</span><br><span class="line">    # start_urls = [&apos;https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/b/ref=sd_allcat_books_l1?ie=UTF8&amp;node=658390051&apos;]</span><br><span class="line">    redis_key = &quot;amazon&quot;</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        #匹配大分类的url地址和小分类的url</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=(&quot;//div[@class=&apos;categoryRefinementsSection&apos;]/ul/li&quot;,)), follow=True),</span><br><span class="line">        #匹配图书的url地址</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=(&quot;//div[@id=&apos;mainResults&apos;]/ul/li//h2/..&quot;,)),callback=&quot;parse_book_detail&quot;),</span><br><span class="line">        #列表页翻页</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=(&quot;//div[@id=&apos;pagn&apos;]&quot;,)),follow=True),</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_book_detail(self,response):</span><br><span class="line">        # with open(response.url.split(&quot;/&quot;)[-1]+&quot;.html&quot;,&quot;w&quot;,encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        #     f.write(response.body.decode())</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        item[&quot;book_title&quot;] = response.xpath(&quot;//span[@id=&apos;productTitle&apos;]/text()&quot;).extract_first()</span><br><span class="line">        item[&quot;book_publish_date&quot;] = response.xpath(&quot;//h1[@id=&apos;title&apos;]/span[last()]/text()&quot;).extract_first()</span><br><span class="line">        item[&quot;book_author&quot;] = response.xpath(&quot;//div[@id=&apos;byline&apos;]/span/a/text()&quot;).extract()</span><br><span class="line">        # item[&quot;book_img&quot;] = response.xpath(&quot;//div[@id=&apos;img-canvas&apos;]/img/@src&quot;).extract_first()</span><br><span class="line">        item[&quot;book_price&quot;] = response.xpath(&quot;//div[@id=&apos;soldByThirdParty&apos;]/span[2]/text()&quot;).extract_first()</span><br><span class="line">        item[&quot;book_cate&quot;] = response.xpath(&quot;//div[@id=&apos;wayfinding-breadcrumbs_feature_div&apos;]/ul/li[not(@class)]/span/a/text()&quot;).extract()</span><br><span class="line">        item[&quot;book_cate&quot;] = [i.strip() for i in item[&quot;book_cate&quot;]]</span><br><span class="line">        item[&quot;book_url&quot;] = response.url</span><br><span class="line">        item[&quot;book_press&quot;] = response.xpath(&quot;//b[text()=&apos;出版社:&apos;]/../text()&quot;).extract_first()</span><br><span class="line">        # item[&quot;book_desc&quot;] = re.findall(r&apos;&lt;noscript&gt;.*?&lt;div&gt;(.*?)&lt;/div&gt;.*?&lt;/noscript&gt;&apos;,response.body.decode(),re.S)</span><br><span class="line">        # item[&quot;book_desc&quot;] = response.xpath(&quot;//noscript/div/text()&quot;).extract()</span><br><span class="line">        # item[&quot;book_desc&quot;] = [i.strip() for i in item[&quot;book_desc&quot;] if len(i.strip())&gt;0 and i!=&apos;海报：&apos;]</span><br><span class="line">        # item[&quot;book_desc&quot;] = item[&quot;book_desc&quot;][0].split(&quot;&lt;br&gt;&quot;,1)[0] if len(item[&quot;book_desc&quot;])&gt;0 else None</span><br><span class="line">        print(item)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;dangdang.py &lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy" scheme="http://jlslzsc.github.io/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>scrapy-京东案例</title>
    <link href="http://jlslzsc.github.io/2019/09/22/scrapy-%E4%BA%AC%E4%B8%9C%E6%A1%88%E4%BE%8B/"/>
    <id>http://jlslzsc.github.io/2019/09/22/scrapy-京东案例/</id>
    <published>2019-09-22T06:46:50.000Z</published>
    <updated>2021-01-11T10:11:40.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>jd.py</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from copy import deepcopy</span><br><span class="line">import urllib,json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class JdSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;jd&apos;</span><br><span class="line">    allowed_domains = [&apos;jd.com&apos;,&apos;p.3.cn&apos;]</span><br><span class="line">    start_urls = [&apos;https://book.jd.com/booksort.html&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        dt_list = response.xpath(&quot;//div[@class=&apos;mc&apos;]/dl/dt&quot;)</span><br><span class="line">        for dt in dt_list:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[&quot;b_cate&quot;] = dt.xpath(&quot;./a/text()&quot;).extract_first()</span><br><span class="line">            em_list = dt.xpath(&quot;./following-sibling::dd[1]/em&quot;)</span><br><span class="line">            for em in em_list:</span><br><span class="line">                item[&quot;s_href&quot;] = em.xpath(&quot;./a/@href&quot;).extract_first()</span><br><span class="line">                item[&quot;s_cate&quot;] = em.xpath(&quot;./a/text()&quot;).extract_first()</span><br><span class="line">                if item[&quot;s_href&quot;] is not None:</span><br><span class="line">                    item[&quot;s_href&quot;] =&quot;https:&quot; + item[&quot;s_href&quot;]</span><br><span class="line">                    yield scrapy.Request(</span><br><span class="line">                        item[&quot;s_href&quot;],</span><br><span class="line">                        callback= self.parse_book_list,</span><br><span class="line">                        meta = &#123;&quot;item&quot; : deepcopy(item)&#125;</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def parse_book_list(self,response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        li_list = response.xpath(&quot;//div[@id=&apos;plist&apos;]/ul/li&quot;)</span><br><span class="line">        for li in li_list:</span><br><span class="line">            item[&quot;book_img&quot;] = li.xpath(&quot;.//div[@class=&apos;p-img&apos;]//img/@src&quot;).extract_first()</span><br><span class="line">            if item[&quot;book_img&quot;] is None:</span><br><span class="line">                item[&quot;book_img&quot;] = li.xpath(&quot;.//div[@class=&apos;p-img&apos;]//img/@data-lazy-img&quot;).extract_first()</span><br><span class="line">            item[&quot;book_img&quot;] = &quot;https:&quot; + item[&quot;book_img&quot;] if item[&quot;book_img&quot;] is not None else None</span><br><span class="line">            item[&quot;book_name&quot;] = li.xpath(&quot;.//div[@class=&apos;p-name&apos;]/a/em/text()&quot;).extract_first().strip()</span><br><span class="line">            item[&quot;book_author&quot;] = li.xpath(&quot;.//span[@class=&apos;author_type_1&apos;]/a/text()&quot;).extract()</span><br><span class="line">            item[&quot;book_press&quot;] = li.xpath(&quot;.//span[@class=&apos;p-bi-store&apos;]/a/@title&quot;).extract_first()</span><br><span class="line">            item[&quot;book_publish_date&quot;] = li.xpath(&quot;.//span[@class=&apos;p-bi-date&apos;]/text()&quot;).extract_first().strip()</span><br><span class="line">            item[&quot;book_sku&quot;] = li.xpath(&quot;./div/@data-sku&quot;).extract_first()</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                &quot;https://p.3.cn/prices/mgets?skuIds=J_&#123;&#125;&quot;.format(item[&quot;book_sku&quot;]),</span><br><span class="line">                callback=self.parse_book_price,</span><br><span class="line">                meta=&#123;&quot;item&quot;: deepcopy(item)&#125;</span><br><span class="line">            )</span><br><span class="line">        #列表页翻页</span><br><span class="line">        next_url = response.xpath(&quot;//a[@class=&apos;pn-next&apos;]/@href&quot;).extract_first()</span><br><span class="line">        if next_url is not None:</span><br><span class="line">            next_url = urllib.parse.urljoin(response.url,next_url)</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                callback=self.parse_book_list,</span><br><span class="line">                meta = &#123;&quot;item&quot;:item&#125;</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def parse_book_price(self, response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        item[&quot;book_price&quot;] = json.loads(response.body.decode())[0][&quot;op&quot;]</span><br><span class="line">        print(item)</span><br></pre></td></tr></table></figure><ul><li><p>setting.py加入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line">REDIS_URL = &quot;redis://127.0.0.1:6379&quot;</span><br><span class="line">LEVEL =&quot;WARNING&quot;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;jd.py&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;lin
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy实例" scheme="http://jlslzsc.github.io/tags/scrapy%E5%AE%9E%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>scrapy-crawl基本流程</title>
    <link href="http://jlslzsc.github.io/2019/09/22/scrapy-crawl%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
    <id>http://jlslzsc.github.io/2019/09/22/scrapy-crawl基本流程/</id>
    <published>2019-09-22T06:38:30.000Z</published>
    <updated>2021-01-11T10:12:06.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>scrapy startproject mySpider</li><li>scrapy genspider –t crawl csdn “csdn.cn”</li><li><img src="//jlslzsc.github.io/2019/09/22/scrapy-crawl基本流程/crawlspider.jpg" alt="crawlspider"></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class CfSpider(CrawlSpider):</span><br><span class="line">    name = &apos;cf&apos;</span><br><span class="line">    allowed_domains = [&apos;circ.gov.cn&apos;]</span><br><span class="line">    start_urls = [&apos;http://www.circ.gov.cn/web/site0/tab5240/module14430/page1.htm&apos;]</span><br><span class="line"></span><br><span class="line">    #定义提取url地址规则</span><br><span class="line">    rules = (</span><br><span class="line">        #LinkExtractor 连接提取器，提取url地址</span><br><span class="line">        #callback 提取出来的url地址的response会交给callback处理</span><br><span class="line">        #follow 当前url地址的响应是够重新进过rules来提取url地址，</span><br><span class="line">        Rule(LinkExtractor(allow=r&apos;/web/site0/tab5240/info\d+\.htm&apos;), callback=&apos;parse_item&apos;),</span><br><span class="line">        Rule(LinkExtractor(allow=r&apos;/web/site0/tab5240/module14430/page\d+\.htm&apos;),follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    #parse函数有特殊功能，不能定义</span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        item[&quot;title&quot;] = re.findall(&quot;&lt;!--TitleStart--&gt;(.*?)&lt;!--TitleEnd--&gt;&quot;,response.body.decode())[0]</span><br><span class="line">        item[&quot;publish_date&quot;] = re.findall(&quot;发布时间：(20\d&#123;2&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&quot;,response.body.decode())[0]</span><br><span class="line">        print(item)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;scrapy startproject mySpider&lt;/li&gt;
&lt;li&gt;scrapy genspider –t crawl csdn “csdn.cn”&lt;/li&gt;
&lt;li&gt;&lt;img src=&quot;//jlslzsc.github.io/2019/09/22/sc
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy" scheme="http://jlslzsc.github.io/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>jdbc连接</title>
    <link href="http://jlslzsc.github.io/2019/09/22/jdbc%E8%BF%9E%E6%8E%A5/"/>
    <id>http://jlslzsc.github.io/2019/09/22/jdbc连接/</id>
    <published>2019-09-22T06:30:08.000Z</published>
    <updated>2021-01-11T10:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="jabc连接"><a href="#jabc连接" class="headerlink" title="jabc连接"></a>jabc连接</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> jdbc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dao</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Connection conn = MySqlConn.getConn();</span><br><span class="line">    <span class="keyword">private</span> Statement stmt;</span><br><span class="line">    <span class="keyword">private</span> ResultSet rs;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Connection <span class="title">getConn</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> conn;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResultSet <span class="title">query</span><span class="params">(String sql)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            stmt = conn.createStatement(ResultSet.TYPE_SCROLL_SENSITIVE,ResultSet.CONCUR_UPDATABLE);</span><br><span class="line">            rs = stmt.executeQuery(sql);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(<span class="string">"Data.executeQuery Error!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> rs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            rs.close();</span><br><span class="line">            stmt.close();</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(<span class="string">"关闭错误！"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> jdbc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySqlConn</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection conn;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Connection <span class="title">getConn</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">            conn = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test"</span>,<span class="string">"root"</span>,<span class="string">"1999"</span>);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(<span class="string">"数据库连接失败！"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> conn;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>alter table usertable auto_increment=2;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;jabc连接&quot;&gt;&lt;a href=&quot;#jabc连接&quot; class=&quot;headerlink&quot; title=&quot;jabc连接&quot;&gt;&lt;/a&gt;jabc连接&lt;/h1&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
      
    
    </summary>
    
      <category term="java" scheme="http://jlslzsc.github.io/categories/java/"/>
    
    
      <category term="jdbc" scheme="http://jlslzsc.github.io/tags/jdbc/"/>
    
  </entry>
  
  <entry>
    <title>scrapy基本流程</title>
    <link href="http://jlslzsc.github.io/2019/09/22/scrapy%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
    <id>http://jlslzsc.github.io/2019/09/22/scrapy基本流程/</id>
    <published>2019-09-22T06:28:16.000Z</published>
    <updated>2021-01-11T10:12:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="scrapy初步使用"><a href="#scrapy初步使用" class="headerlink" title="scrapy初步使用"></a>scrapy初步使用</h1><ul><li><p>scrapy startproject mySpider</p></li><li><p>scrapy genspider itcast “itcast.cn”</p></li><li><p>items中添加想要的字典的键</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YangguangItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    href = scrapy.Field()</span><br><span class="line">    publish_date = scrapy.Field()</span><br><span class="line">    content_img = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure></li><li><p>piplines中可处理东西</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class YangguangPipeline(object):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        item[&quot;content&quot;] = self.process_content(item[&quot;content&quot;])</span><br><span class="line">        print(item)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def process_content(self,content):</span><br><span class="line">        content = [re.sub(r&quot;\xa0|\s&quot;,&quot;&quot;,i)  for i in content]</span><br><span class="line">        content = [i for i in content if len(i)&gt;0]</span><br><span class="line">        return content</span><br></pre></td></tr></table></figure></li></ul><ul><li>setting中加入一行,去除警告</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LEVEL =&quot;WARNING&quot;</span><br></pre></td></tr></table></figure><ul><li><p>yg.py中写</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class YgSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;yg&apos;</span><br><span class="line">    allowed_domains = [&apos;wz.sun0769.com&apos;]</span><br><span class="line">    start_urls = [&apos;http://wz.sun0769.com/index.php/question/questionType?type=4&amp;page=0&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        tr_list = response.xpath(&quot;//div[@class=&apos;greyframe&apos;]/table[2]/tr/td/table/tr&quot;)</span><br><span class="line">        for tr in tr_list:</span><br><span class="line">            item = YangguangItem()</span><br><span class="line">            item[&quot;title&quot;] = tr.xpath(&quot;./td[2]/a[@class=&apos;news14&apos;]/@title&quot;).extract_first()</span><br><span class="line">            item[&quot;href&quot;] = tr.xpath(&quot;./td[2]/a[@class=&apos;news14&apos;]/@href&quot;).extract_first()</span><br><span class="line">            item[&quot;publish_date&quot;] = tr.xpath(&quot;./td[last()]/text()&quot;).extract_first()</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                item[&quot;href&quot;],</span><br><span class="line">                callback=self.parse_detail,</span><br><span class="line">                meta=&#123;&quot;item&quot;:item&#125;</span><br><span class="line">            )</span><br><span class="line">        next_url = response.xpath(&quot;//a[text()=&apos;&gt;&apos;]/@href&quot;).extract_first()</span><br><span class="line">        if next_url is not None:</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                callback=self.parse</span><br><span class="line">            )</span><br><span class="line">    def parse_detail(self,response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        item[&quot;content&quot;] = response.xpath(&quot;//td[@class=&apos;txt16_3&apos;]/text()&quot;).extract()</span><br><span class="line">        item[&quot;content_img&quot;] = response.xpath(&quot;//td[@class=&apos;txt16_3&apos;]//img/@src&quot;).extract()</span><br><span class="line">        item[&quot;content_img&quot;] = [&quot;http://http://wz.sun0769.com&quot; + i for i in item[&quot;content_img&quot;]]</span><br><span class="line">        yield item</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;scrapy初步使用&quot;&gt;&lt;a href=&quot;#scrapy初步使用&quot; class=&quot;headerlink&quot; title=&quot;scrapy初步使用&quot;&gt;&lt;/a&gt;scrapy初步使用&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;scrapy startproject mySpider&lt;
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy" scheme="http://jlslzsc.github.io/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>scrapy-相关</title>
    <link href="http://jlslzsc.github.io/2019/09/22/scrapy-%E7%9B%B8%E5%85%B3/"/>
    <id>http://jlslzsc.github.io/2019/09/22/scrapy-相关/</id>
    <published>2019-09-22T06:21:58.000Z</published>
    <updated>2021-01-11T10:11:56.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="logging-模块的使用"><a href="#logging-模块的使用" class="headerlink" title="logging 模块的使用"></a>logging 模块的使用</h3><ul><li>scrapy<ul><li>settings中设置LOG_LEVEL=“WARNING”</li><li>settings中设置LOG_FILE=”./a.log”  #设置日志保存的位置，设置会后终端不会显示日志内容</li><li>import logging,实例化logger的方式在任何文件中使用logger输出内容</li></ul></li><li><code>logger = logging.getLogger(__name__)       logger.warning(item)</code></li><li>普通项目中<ul><li>import logging</li><li>logging.basicConfig(…) #设置日志输出的样式，格式</li><li>实例化一个<code>logger=logging.getLogger(__name__)</code></li><li>在任何py文件中调用logger即可</li></ul></li></ul><p><img src="//jlslzsc.github.io/2019/09/22/scrapy-相关/day08%E6%80%BB%E7%BB%93.jpg" alt="day08总结"></p><h3 id="crawlspider的使用"><a href="#crawlspider的使用" class="headerlink" title="crawlspider的使用"></a>crawlspider的使用</h3><ul><li><p>常见爬虫 scrapy genspider -t crawl 爬虫名 allow_domain</p></li><li><p>指定start_url，对应的响应会进过rules提取url地址</p></li><li><p>完善rules，添加Rule <code>Rule(LinkExtractor(allow=r&#39;/web/site0/tab5240/info\d+\.htm&#39;), callback=&#39;parse_item&#39;),</code></p></li><li><p>注意点:</p><ul><li><p>url地址不完整，crawlspider会自动补充完整之后在请求</p></li><li><p>parse函数不能定义，他有特殊的功能需要实现</p></li><li><p>callback：连接提取器提取出来的url地址对应的响应交给他处理</p></li><li><p>follow：连接提取器提取出来的url地址对应的响应是否继续被rules来过滤</p><p><img src="//jlslzsc.github.io/2019/09/22/scrapy-相关/day10%E6%80%BB%E7%BB%93.jpg" alt="day10总结"></p><p><img src="//jlslzsc.github.io/2019/09/22/scrapy-相关/day09%E6%80%BB%E7%BB%93.jpg" alt="day09总结"></p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;logging-模块的使用&quot;&gt;&lt;a href=&quot;#logging-模块的使用&quot; class=&quot;headerlink&quot; title=&quot;logging 模块的使用&quot;&gt;&lt;/a&gt;logging 模块的使用&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;scrapy&lt;ul&gt;
&lt;li&gt;setti
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="scrapy" scheme="http://jlslzsc.github.io/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>验证码与自动化</title>
    <link href="http://jlslzsc.github.io/2019/09/22/%E9%AA%8C%E8%AF%81%E7%A0%81%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    <id>http://jlslzsc.github.io/2019/09/22/验证码与自动化/</id>
    <published>2019-09-22T06:21:02.000Z</published>
    <updated>2021-01-11T10:10:40.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="验证码的识别"><a href="#验证码的识别" class="headerlink" title="验证码的识别"></a>验证码的识别</h4><ul><li>url不变，验证码不变<ul><li>请求验证码的地址，获得相应，识别</li></ul></li><li>url不变，验证码会变<ul><li>思路：对方服务器返回验证码的时候，会和每个用户的信息和验证码进行一个对应，之后，在用户发送post请求的时候，会对比post请求中法的验证码和当前用户真正的存储在服务器端的验证码是否相同</li><li>1.实例化session</li><li>2.使用seesion请求登录页面，获取验证码的地址</li><li>3.使用session请求验证码，识别</li><li>4.使用session发送post请求’</li></ul></li><li>使用selenium登录，遇到验证码<ul><li>url不变，验证码不变，同上</li><li>url不变，验证码会变<ul><li>1.selenium请求登录页面，同时拿到验证码的地址</li><li>2.获取登录页面中driver中的cookie，交给requests模块发送验证码的请求，识别</li><li>3.输入验证码，点击登录</li></ul></li></ul></li></ul><h3 id="selenium使用的注意点"><a href="#selenium使用的注意点" class="headerlink" title="selenium使用的注意点"></a>selenium使用的注意点</h3><ul><li>获取文本和获取属性<ul><li>先定位到元素，然后调用<code>.text</code>或者<code>get_attribute</code>方法来去</li></ul></li><li>selenium获取的页面数据是浏览器中elements的内容</li><li>find_element和find_elements的区别<ul><li>find_element返回一个element，如果没有会报错</li><li>find_elements返回一个列表，没有就是空列表</li><li>在判断是否有下一页的时候，使用find_elements来根据结果的列表长度来判断</li></ul></li><li>如果页面中含有iframe、frame，需要先调用driver.switch_to.frame的方法切换到frame中才能定位元素</li><li>selenium请求第一页的时候回等待页面加载完了之后在获取数据，但是在点击翻页之后，hi直接获取数据，此时可能会报错，因为数据还没有加载出来，需要time.sleep(3)</li><li>selenium中find_element_by_class_name智能接收一个class对应的一个值，不能传入多个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:&quot;$name&quot;,counter:&#123;$sum:2&#125;&#125;&#125;)</span><br><span class="line"></span><br><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:null,counter:&#123;$sum:1&#125;&#125;&#125;)</span><br><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:&quot;$gender&quot;,name:&#123;$push:&quot;$name&quot;&#125;&#125;&#125;)</span><br><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:&quot;$gender&quot;,name:&#123;$push:&quot;$$ROOT&quot;&#125;&#125;&#125;)</span><br><span class="line">db.tv3.aggregate(</span><br><span class="line">  &#123;$group:&#123;_id:&#123;&quot;country&quot;:&quot;$country&quot;,province:&quot;$province&quot;,userid:&quot;$userid&quot;&#125;&#125;&#125;,</span><br><span class="line">  &#123;$group:&#123;_id:&#123;country:&quot;$_id.country&quot;,province:&quot;$_id.province&quot;&#125;,count:&#123;$sum:1&#125;&#125;&#125;,</span><br><span class="line">  &#123;$project:&#123;country:&quot;$_id.country&quot;,province:&quot;$_id.province&quot;,count:&quot;$count&quot;,_id:0&#125;&#125;</span><br><span class="line">  )</span><br><span class="line">db.stu.aggregate(</span><br><span class="line"></span><br><span class="line">  &#123;$match:&#123;age:&#123;$gt:20&#125;&#125;&#125;,</span><br><span class="line">  &#123;$group:&#123;_id:&quot;$gender&quot;,count:&#123;$sum:1&#125;&#125;&#125;</span><br><span class="line">  )</span><br><span class="line">db.t2.aggregate(</span><br><span class="line">  &#123;$unwind:&quot;$size&quot;&#125;</span><br><span class="line">  )</span><br><span class="line">db.t3.aggregate(</span><br><span class="line">  &#123;$unwind:&quot;$tags&quot;&#125;,</span><br><span class="line">  &#123;$group:&#123;_id:null,count:&#123;$sum:1&#125;&#125;&#125;</span><br><span class="line">  )</span><br><span class="line">db.t3.aggregate(</span><br><span class="line">  &#123;$unwind:&#123;path:&quot;$size&quot;,preserveNullAndEmptyArrays:true&#125;&#125;</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;验证码的识别&quot;&gt;&lt;a href=&quot;#验证码的识别&quot; class=&quot;headerlink&quot; title=&quot;验证码的识别&quot;&gt;&lt;/a&gt;验证码的识别&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;url不变，验证码不变&lt;ul&gt;
&lt;li&gt;请求验证码的地址，获得相应，识别&lt;/li&gt;
&lt;/ul&gt;

      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>爬虫基本操作</title>
    <link href="http://jlslzsc.github.io/2019/09/22/%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://jlslzsc.github.io/2019/09/22/爬虫基本操作/</id>
    <published>2019-09-22T06:20:10.000Z</published>
    <updated>2021-01-11T10:10:12.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="xpath的包含"><a href="#xpath的包含" class="headerlink" title="xpath的包含"></a>xpath的包含</h3><ul><li><code>//div[contains(@class,&#39;i&#39;)]</code></li></ul><h3 id="实现爬虫的套路"><a href="#实现爬虫的套路" class="headerlink" title="实现爬虫的套路"></a>实现爬虫的套路</h3><ul><li>准备url<ul><li>准备start_url<ul><li>url地址规律不明显，总数不确定</li><li>通过代码提取下一页的url<ul><li>xpath</li><li>寻找url地址，部分参数在当前的响应中（比如，当前页码数和总的页码数在当前的响应中）</li></ul></li></ul></li><li>准备url_list<ul><li>页码总数明确</li><li>url地址规律明显</li></ul></li></ul></li><li>发送请求，获取响应<ul><li>添加随机的User-Agent,反反爬虫</li><li>添加随机的代理ip，反反爬虫</li><li>在对方判断出我们是爬虫之后，应该添加更多的headers字段，包括cookie</li><li>cookie的处理可以使用session来解决</li><li>准备一堆能用的cookie，组成cookie池<ul><li>如果不登录<ul><li>准备刚开始能够成功请求对方网站的cookie，即接收对方网站设置在response的cookie</li><li>下一次请求的时候，使用之前的列表中的cookie来请求</li></ul></li><li>如果登录<ul><li>准备多个账号</li><li>使用程序获取每个账号的cookie</li><li>之后请求登录之后才能访问的网站随机的选择cookie</li></ul></li></ul></li></ul></li><li>提取数据<ul><li>确定数据的位置<ul><li>如果数据在当前的url地址中<ul><li>提取的是列表页的数据<ul><li>直接请求列表页的url地址，不用进入详情页</li></ul></li><li>提取的是详情页的数据<ul><li><ol><li>确定url</li></ol></li><li><ol start="2"><li>发送请求</li></ol></li><li><ol start="3"><li>提取数据</li></ol></li><li><ol start="4"><li>返回</li></ol></li></ul></li></ul></li><li>如果数据不在当前的url地址中<ul><li>在其他的响应中，寻找数据的位置<ul><li><ol><li>从network中从上往下找</li></ol></li><li><ol start="2"><li>使用chrome中的过滤条件，选择出了js,css,img之外的按钮</li></ol></li><li><ol start="3"><li>使用chrome的search all file，搜索数字和英文</li></ol></li></ul></li></ul></li></ul></li><li>数据的提取<ul><li>xpath,从html中提取整块的数据，先分组，之后每一组再提取</li><li>re，提取max_time,price,html中的json字符串</li><li>json</li></ul></li></ul></li></ul><ul><li>保存<ul><li>保存在本地，text,json,csv</li><li>保存在数据库</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;xpath的包含&quot;&gt;&lt;a href=&quot;#xpath的包含&quot; class=&quot;headerlink&quot; title=&quot;xpath的包含&quot;&gt;&lt;/a&gt;xpath的包含&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;//div[contains(@class,&amp;#39;i&amp;#39;)
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://jlslzsc.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>正则与XPATH</title>
    <link href="http://jlslzsc.github.io/2019/09/22/%E6%AD%A3%E5%88%99%E4%B8%8EXPATH/"/>
    <id>http://jlslzsc.github.io/2019/09/22/正则与XPATH/</id>
    <published>2019-09-22T06:19:28.000Z</published>
    <updated>2021-01-11T10:11:14.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="正则使用的注意点"><a href="#正则使用的注意点" class="headerlink" title="正则使用的注意点"></a>正则使用的注意点</h3><ul><li><code>re.findall(&quot;a(.*?)b&quot;,&quot;str&quot;)</code>,能够返回括号中的内容,括号前后的内容起到定位和过滤的效果</li><li>原始字符串r，待匹配字符串中有反斜杠的时候，使用r能够忽视反斜杠带来的转义的效果</li><li>点号默认情况匹配不到<code>\n</code></li><li><code>\s</code>能够匹配空白字符，不仅仅包含空格，还有<code>\t|\r\n</code></li></ul><h3 id="xpath学习重点"><a href="#xpath学习重点" class="headerlink" title="xpath学习重点"></a>xpath学习重点</h3><ul><li>使用xpath helper或者是chrome中的copy xpath都是从element中提取的数据，但是爬虫获取的是url对应的响应，往往和elements不一样</li><li>获取文本<ul><li><code>a/text()</code> 获取a下的文本</li><li><code>a//text()</code> 获取a下的所有标签的文本</li><li><code>//a[text()=&#39;下一页&#39;]</code> 选择文本为下一页三个字的a标签</li></ul></li><li><code>@符号</code><ul><li><code>a/@href</code></li><li><code>//ul[@id=&quot;detail-list&quot;]</code></li></ul></li><li><code>//</code><ul><li>在xpath最前面表示从当前html中任意位置开始选择</li><li><code>li//a</code> 表示的是li下任何一个标签</li></ul></li></ul><h3 id="lxml使用注意点"><a href="#lxml使用注意点" class="headerlink" title="lxml使用注意点"></a>lxml使用注意点</h3><ul><li>lxml能够修正HTML代码，但是可能会改错了<ul><li>使用etree.tostring观察修改之后的html的样子，根据修改之后的html字符串写xpath</li></ul></li><li>lxml 能够接受bytes和str的字符串</li><li>提取页面数据的思路<ul><li>先分组，渠道一个包含分组标签的列表</li><li>遍历，取其中每一组进行数据的提取，不会造成数据的对应错乱</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;正则使用的注意点&quot;&gt;&lt;a href=&quot;#正则使用的注意点&quot; class=&quot;headerlink&quot; title=&quot;正则使用的注意点&quot;&gt;&lt;/a&gt;正则使用的注意点&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;re.findall(&amp;quot;a(.*?)b&amp;quot;,&amp;qu
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫技巧" scheme="http://jlslzsc.github.io/tags/%E7%88%AC%E8%99%AB%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>寻找post地址</title>
    <link href="http://jlslzsc.github.io/2019/09/22/%E5%AF%BB%E6%89%BEpost%E5%9C%B0%E5%9D%80/"/>
    <id>http://jlslzsc.github.io/2019/09/22/寻找post地址/</id>
    <published>2019-09-22T06:18:46.000Z</published>
    <updated>2021-01-11T10:10:30.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="寻找登录的post地址"><a href="#寻找登录的post地址" class="headerlink" title="寻找登录的post地址"></a>寻找登录的post地址</h3><ul><li>在form表单中寻找action对应的url地址<ul><li>post的数据是input标签中name的值作为键，真正的用户名密码作为值的字典，post的url地址就是action对应的url地址</li></ul></li><li>抓包，寻找登录的url地址<ul><li>勾选perserve log按钮，防止页面跳转找不到url</li><li>寻找post数据，确定参数<ul><li>参数不会变，直接用，比如密码不是动态加密的时候</li><li>参数会变<ul><li>参数在当前的响应中</li><li>通过js生成</li></ul></li></ul></li></ul></li></ul><h3 id="定位想要的js"><a href="#定位想要的js" class="headerlink" title="定位想要的js"></a>定位想要的js</h3><ul><li>选择会触发js时间的按钮，点击event listener，找到js的位置</li><li>通过chrome中的search all file来搜索url中关键字</li><li>添加断点的方式来查看js的操作，通过python来进行同样的操作</li></ul><h3 id="安装第三方模块"><a href="#安装第三方模块" class="headerlink" title="安装第三方模块"></a>安装第三方模块</h3><ul><li>pip install retrying</li><li>下载源码解码，进入解压后的目录，<code>python setup.py install</code></li><li><code>***.whl</code> 安装方法 <code>pip install ***.whl</code></li></ul><h3 id="json使用注意点"><a href="#json使用注意点" class="headerlink" title="json使用注意点"></a>json使用注意点</h3><ul><li>json中的字符串都是双引号引起来的<ul><li>如果不是双引号<ul><li>eval：能实现简单的字符串和python类型的转化</li><li>replace：把单引号替换为双引号</li></ul></li></ul></li><li>往一个文件中写入多个json串，不再是一个json串，不能直接读取<ul><li>一行写一个json串，按照行来读取</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;寻找登录的post地址&quot;&gt;&lt;a href=&quot;#寻找登录的post地址&quot; class=&quot;headerlink&quot; title=&quot;寻找登录的post地址&quot;&gt;&lt;/a&gt;寻找登录的post地址&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在form表单中寻找action对应的url地址&lt;ul&gt;
      
    
    </summary>
    
      <category term="爬虫" scheme="http://jlslzsc.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
</feed>

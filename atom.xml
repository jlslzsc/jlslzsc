<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>冷眸</title>
  
  <subtitle>丞</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-22T07:24:40.513Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhou ShuCheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>scrapy相关配置使用</title>
    <link href="http://yoursite.com/2019/09/22/scrapy%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/09/22/scrapy相关配置使用/</id>
    <published>2019-09-22T07:10:36.795Z</published>
    <updated>2019-09-22T07:24:40.513Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th>item使用</th></tr></thead><tbody><tr><td><img src="//yoursite.com/2019/09/22/scrapy相关配置使用/scrapy%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%5Citem%E4%BD%BF%E7%94%A8.jpg" alt="item使用"></td></tr><tr><td>item使用2</td></tr><tr><td><img src="//yoursite.com/2019/09/22/scrapy相关配置使用/scrapy%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%5Citem%E4%BD%BF%E7%94%A82.jpg" alt="item使用2"></td></tr><tr><td>scrapy翻页</td></tr><tr><td><img src="//yoursite.com/2019/09/22/scrapy相关配置使用/scrapy%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%5Cscrapy%E7%BF%BB%E9%A1%B5.jpg" alt="scrapy翻页"></td></tr><tr><td>scrapy post请求</td></tr><tr><td>![scrapy post请求](scrapy相关配置使用\scrapy post请求.jpg)</td></tr><tr><td>scrapy post 请求2</td></tr><tr><td>![scrapy post 请求2](scrapy相关配置使用\scrapy post 请求2.jpg)</td></tr><tr><td>scrapy cookies登录</td></tr><tr><td>![scrapy cookies登录](scrapy相关配置使用\scrapy cookies登录.jpg)</td></tr><tr><td>scrapy cookies 登录1</td></tr><tr><td>![scrapy cookies 登录1](scrapy相关配置使用\scrapy cookies 登录1.jpg)</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;item使用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;//yoursite.com/2019/09/22/scrapy相关配置使用/scrapy%E7%9B%B8%E5%85%B
      
    
    </summary>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>scrapy与scrapy crawl分布式案例</title>
    <link href="http://yoursite.com/2019/09/22/scrapy%E4%B8%8Escrapy-crawl%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2019/09/22/scrapy与scrapy-crawl分布式案例/</id>
    <published>2019-09-22T06:49:37.203Z</published>
    <updated>2019-09-22T06:50:48.471Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>dangdang.py </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy_redis.spiders import RedisSpider</span><br><span class="line">from copy import deepcopy</span><br><span class="line">import urllib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DangdangSpider(RedisSpider):</span><br><span class="line">    name = &apos;dangdang&apos;</span><br><span class="line">    allowed_domains = [&apos;dangdang.com&apos;]</span><br><span class="line">    # start_urls = [&apos;http://book.dangdang.com/&apos;]</span><br><span class="line">    redis_key = &quot;dangdang&quot;</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        #大分类分组</span><br><span class="line">        div_list = response.xpath(&quot;//div[@class=&apos;con flq_body&apos;]/div&quot;)</span><br><span class="line">        for div in div_list:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[&quot;b_cate&quot;] = div.xpath(&quot;./dl/dt//text()&quot;).extract()</span><br><span class="line">            item[&quot;b_cate&quot;] = [i.strip() for i in item[&quot;b_cate&quot;] if len(i.strip())&gt;0]</span><br><span class="line">            #中间分类分组</span><br><span class="line">            dl_list = div.xpath(&quot;./div//dl[@class=&apos;inner_dl&apos;]&quot;)</span><br><span class="line">            for dl in dl_list:</span><br><span class="line">                item[&quot;m_cate&quot;] = dl.xpath(&quot;./dt//text()&quot;).extract()</span><br><span class="line">                item[&quot;m_cate&quot;] = [i.strip() for i in item[&quot;m_cate&quot;] if len(i.strip())&gt;0][0]</span><br><span class="line">                #小分类分组</span><br><span class="line">                a_list = dl.xpath(&quot;./dd/a&quot;)</span><br><span class="line">                for a in a_list:</span><br><span class="line">                    item[&quot;s_href&quot;] = a.xpath(&quot;./@href&quot;).extract_first()</span><br><span class="line">                    item[&quot;s_cate&quot;] = a.xpath(&quot;./text()&quot;).extract_first()</span><br><span class="line">                    if item[&quot;s_href&quot;] is not None:</span><br><span class="line">                        yield scrapy.Request(</span><br><span class="line">                            item[&quot;s_href&quot;],</span><br><span class="line">                            callback=self.parse_book_list,</span><br><span class="line">                            meta = &#123;&quot;item&quot;:deepcopy(item)&#125;</span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line">    def parse_book_list(self,response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        li_list = response.xpath(&quot;//ul[@class=&apos;bigimg&apos;]/li&quot;)</span><br><span class="line">        for li in li_list:</span><br><span class="line">            item[&quot;book_img&quot;] = li.xpath(&quot;./a[@class=&apos;pic&apos;]/img/@src&quot;).extract_first()</span><br><span class="line">            if item[&quot;book_img&quot;] == &quot;images/model/guan/url_none.png&quot;:</span><br><span class="line">                item[&quot;book_img&quot;] = li.xpath(&quot;./a[@class=&apos;pic&apos;]/img/@data-original&quot;).extract_first()</span><br><span class="line">            item[&quot;book_name&quot;] = li.xpath(&quot;./p[@class=&apos;name&apos;]/a/@title&quot;).extract_first()</span><br><span class="line">            item[&quot;book_desc&quot;] = li.xpath(&quot;./p[@class=&apos;detail&apos;]/text()&quot;).extract_first()</span><br><span class="line">            item[&quot;book_price&quot;] = li.xpath(&quot;.//span[@class=&apos;search_now_price&apos;]/text()&quot;).extract_first()</span><br><span class="line">            item[&quot;book_author&quot;] = li.xpath(&quot;./p[@class=&apos;search_book_author&apos;]/span[1]/a/text()&quot;).extract()</span><br><span class="line">            item[&quot;book_publish_date&quot;] = li.xpath(&quot;./p[@class=&apos;search_book_author&apos;]/span[2]/text()&quot;).extract_first()</span><br><span class="line">            item[&quot;book_press&quot;] = li.xpath(&quot;./p[@class=&apos;search_book_author&apos;]/span[3]/a/text()&quot;).extract_first()</span><br><span class="line">            print(item)</span><br><span class="line">        #下一页</span><br><span class="line">        next_url = response.xpath(&quot;//li[@class=&apos;next&apos;]/a/@href&quot;).extract_first()</span><br><span class="line">        if next_url is not None:</span><br><span class="line">            next_url = urllib.parse.urljoin(response.url,next_url)</span><br><span class="line">            yield  scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                callback=self.parse_book_list,</span><br><span class="line">                meta = &#123;&quot;item&quot;:item&#125;</span><br><span class="line">            )</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>amazno.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">class AmazonSpider(RedisCrawlSpider):</span><br><span class="line">    name = &apos;amazon&apos;</span><br><span class="line">    allowed_domains = [&apos;amazon.cn&apos;]</span><br><span class="line">    # start_urls = [&apos;https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/b/ref=sd_allcat_books_l1?ie=UTF8&amp;node=658390051&apos;]</span><br><span class="line">    redis_key = &quot;amazon&quot;</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        #匹配大分类的url地址和小分类的url</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=(&quot;//div[@class=&apos;categoryRefinementsSection&apos;]/ul/li&quot;,)), follow=True),</span><br><span class="line">        #匹配图书的url地址</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=(&quot;//div[@id=&apos;mainResults&apos;]/ul/li//h2/..&quot;,)),callback=&quot;parse_book_detail&quot;),</span><br><span class="line">        #列表页翻页</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=(&quot;//div[@id=&apos;pagn&apos;]&quot;,)),follow=True),</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_book_detail(self,response):</span><br><span class="line">        # with open(response.url.split(&quot;/&quot;)[-1]+&quot;.html&quot;,&quot;w&quot;,encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        #     f.write(response.body.decode())</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        item[&quot;book_title&quot;] = response.xpath(&quot;//span[@id=&apos;productTitle&apos;]/text()&quot;).extract_first()</span><br><span class="line">        item[&quot;book_publish_date&quot;] = response.xpath(&quot;//h1[@id=&apos;title&apos;]/span[last()]/text()&quot;).extract_first()</span><br><span class="line">        item[&quot;book_author&quot;] = response.xpath(&quot;//div[@id=&apos;byline&apos;]/span/a/text()&quot;).extract()</span><br><span class="line">        # item[&quot;book_img&quot;] = response.xpath(&quot;//div[@id=&apos;img-canvas&apos;]/img/@src&quot;).extract_first()</span><br><span class="line">        item[&quot;book_price&quot;] = response.xpath(&quot;//div[@id=&apos;soldByThirdParty&apos;]/span[2]/text()&quot;).extract_first()</span><br><span class="line">        item[&quot;book_cate&quot;] = response.xpath(&quot;//div[@id=&apos;wayfinding-breadcrumbs_feature_div&apos;]/ul/li[not(@class)]/span/a/text()&quot;).extract()</span><br><span class="line">        item[&quot;book_cate&quot;] = [i.strip() for i in item[&quot;book_cate&quot;]]</span><br><span class="line">        item[&quot;book_url&quot;] = response.url</span><br><span class="line">        item[&quot;book_press&quot;] = response.xpath(&quot;//b[text()=&apos;出版社:&apos;]/../text()&quot;).extract_first()</span><br><span class="line">        # item[&quot;book_desc&quot;] = re.findall(r&apos;&lt;noscript&gt;.*?&lt;div&gt;(.*?)&lt;/div&gt;.*?&lt;/noscript&gt;&apos;,response.body.decode(),re.S)</span><br><span class="line">        # item[&quot;book_desc&quot;] = response.xpath(&quot;//noscript/div/text()&quot;).extract()</span><br><span class="line">        # item[&quot;book_desc&quot;] = [i.strip() for i in item[&quot;book_desc&quot;] if len(i.strip())&gt;0 and i!=&apos;海报：&apos;]</span><br><span class="line">        # item[&quot;book_desc&quot;] = item[&quot;book_desc&quot;][0].split(&quot;&lt;br&gt;&quot;,1)[0] if len(item[&quot;book_desc&quot;])&gt;0 else None</span><br><span class="line">        print(item)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;dangdang.py &lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;
      
    
    </summary>
    
      <category term="案例" scheme="http://yoursite.com/categories/%E6%A1%88%E4%BE%8B/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>scrapy-京东案例</title>
    <link href="http://yoursite.com/2019/09/22/scrapy-%E4%BA%AC%E4%B8%9C%E6%A1%88%E4%BE%8B/"/>
    <id>http://yoursite.com/2019/09/22/scrapy-京东案例/</id>
    <published>2019-09-22T06:46:51.914Z</published>
    <updated>2019-09-22T06:48:33.791Z</updated>
    
    <content type="html"><![CDATA[<ul><li>jd.py</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from copy import deepcopy</span><br><span class="line">import urllib,json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class JdSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;jd&apos;</span><br><span class="line">    allowed_domains = [&apos;jd.com&apos;,&apos;p.3.cn&apos;]</span><br><span class="line">    start_urls = [&apos;https://book.jd.com/booksort.html&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        dt_list = response.xpath(&quot;//div[@class=&apos;mc&apos;]/dl/dt&quot;)</span><br><span class="line">        for dt in dt_list:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[&quot;b_cate&quot;] = dt.xpath(&quot;./a/text()&quot;).extract_first()</span><br><span class="line">            em_list = dt.xpath(&quot;./following-sibling::dd[1]/em&quot;)</span><br><span class="line">            for em in em_list:</span><br><span class="line">                item[&quot;s_href&quot;] = em.xpath(&quot;./a/@href&quot;).extract_first()</span><br><span class="line">                item[&quot;s_cate&quot;] = em.xpath(&quot;./a/text()&quot;).extract_first()</span><br><span class="line">                if item[&quot;s_href&quot;] is not None:</span><br><span class="line">                    item[&quot;s_href&quot;] =&quot;https:&quot; + item[&quot;s_href&quot;]</span><br><span class="line">                    yield scrapy.Request(</span><br><span class="line">                        item[&quot;s_href&quot;],</span><br><span class="line">                        callback= self.parse_book_list,</span><br><span class="line">                        meta = &#123;&quot;item&quot; : deepcopy(item)&#125;</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def parse_book_list(self,response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        li_list = response.xpath(&quot;//div[@id=&apos;plist&apos;]/ul/li&quot;)</span><br><span class="line">        for li in li_list:</span><br><span class="line">            item[&quot;book_img&quot;] = li.xpath(&quot;.//div[@class=&apos;p-img&apos;]//img/@src&quot;).extract_first()</span><br><span class="line">            if item[&quot;book_img&quot;] is None:</span><br><span class="line">                item[&quot;book_img&quot;] = li.xpath(&quot;.//div[@class=&apos;p-img&apos;]//img/@data-lazy-img&quot;).extract_first()</span><br><span class="line">            item[&quot;book_img&quot;] = &quot;https:&quot; + item[&quot;book_img&quot;] if item[&quot;book_img&quot;] is not None else None</span><br><span class="line">            item[&quot;book_name&quot;] = li.xpath(&quot;.//div[@class=&apos;p-name&apos;]/a/em/text()&quot;).extract_first().strip()</span><br><span class="line">            item[&quot;book_author&quot;] = li.xpath(&quot;.//span[@class=&apos;author_type_1&apos;]/a/text()&quot;).extract()</span><br><span class="line">            item[&quot;book_press&quot;] = li.xpath(&quot;.//span[@class=&apos;p-bi-store&apos;]/a/@title&quot;).extract_first()</span><br><span class="line">            item[&quot;book_publish_date&quot;] = li.xpath(&quot;.//span[@class=&apos;p-bi-date&apos;]/text()&quot;).extract_first().strip()</span><br><span class="line">            item[&quot;book_sku&quot;] = li.xpath(&quot;./div/@data-sku&quot;).extract_first()</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                &quot;https://p.3.cn/prices/mgets?skuIds=J_&#123;&#125;&quot;.format(item[&quot;book_sku&quot;]),</span><br><span class="line">                callback=self.parse_book_price,</span><br><span class="line">                meta=&#123;&quot;item&quot;: deepcopy(item)&#125;</span><br><span class="line">            )</span><br><span class="line">        #列表页翻页</span><br><span class="line">        next_url = response.xpath(&quot;//a[@class=&apos;pn-next&apos;]/@href&quot;).extract_first()</span><br><span class="line">        if next_url is not None:</span><br><span class="line">            next_url = urllib.parse.urljoin(response.url,next_url)</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                callback=self.parse_book_list,</span><br><span class="line">                meta = &#123;&quot;item&quot;:item&#125;</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def parse_book_price(self, response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        item[&quot;book_price&quot;] = json.loads(response.body.decode())[0][&quot;op&quot;]</span><br><span class="line">        print(item)</span><br></pre></td></tr></table></figure><ul><li><p>setting.py加入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line">REDIS_URL = &quot;redis://127.0.0.1:6379&quot;</span><br><span class="line">LEVEL =&quot;WARNING&quot;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;jd.py&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;lin
      
    
    </summary>
    
      <category term="实例" scheme="http://yoursite.com/categories/%E5%AE%9E%E4%BE%8B/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>scrapy-crawl基本流程</title>
    <link href="http://yoursite.com/2019/09/22/scrapy-crawl%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/09/22/scrapy-crawl基本流程/</id>
    <published>2019-09-22T06:38:31.386Z</published>
    <updated>2019-09-22T07:23:52.322Z</updated>
    
    <content type="html"><![CDATA[<ul><li>scrapy startproject mySpider</li><li>scrapy genspider –t crawl csdn “csdn.cn”</li><li><img src="//yoursite.com/2019/09/22/scrapy-crawl基本流程/scrapy-crawl%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%5Ccrawlspider.jpg" alt="crawlspider"></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class CfSpider(CrawlSpider):</span><br><span class="line">    name = &apos;cf&apos;</span><br><span class="line">    allowed_domains = [&apos;circ.gov.cn&apos;]</span><br><span class="line">    start_urls = [&apos;http://www.circ.gov.cn/web/site0/tab5240/module14430/page1.htm&apos;]</span><br><span class="line"></span><br><span class="line">    #定义提取url地址规则</span><br><span class="line">    rules = (</span><br><span class="line">        #LinkExtractor 连接提取器，提取url地址</span><br><span class="line">        #callback 提取出来的url地址的response会交给callback处理</span><br><span class="line">        #follow 当前url地址的响应是够重新进过rules来提取url地址，</span><br><span class="line">        Rule(LinkExtractor(allow=r&apos;/web/site0/tab5240/info\d+\.htm&apos;), callback=&apos;parse_item&apos;),</span><br><span class="line">        Rule(LinkExtractor(allow=r&apos;/web/site0/tab5240/module14430/page\d+\.htm&apos;),follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    #parse函数有特殊功能，不能定义</span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        item[&quot;title&quot;] = re.findall(&quot;&lt;!--TitleStart--&gt;(.*?)&lt;!--TitleEnd--&gt;&quot;,response.body.decode())[0]</span><br><span class="line">        item[&quot;publish_date&quot;] = re.findall(&quot;发布时间：(20\d&#123;2&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&quot;,response.body.decode())[0]</span><br><span class="line">        print(item)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;scrapy startproject mySpider&lt;/li&gt;
&lt;li&gt;scrapy genspider –t crawl csdn “csdn.cn”&lt;/li&gt;
&lt;li&gt;&lt;img src=&quot;//yoursite.com/2019/09/22/scrapy-
      
    
    </summary>
    
      <category term="基本操作" scheme="http://yoursite.com/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>django的坑</title>
    <link href="http://yoursite.com/2019/09/22/django%E7%9A%84%E5%9D%91/"/>
    <id>http://yoursite.com/2019/09/22/django的坑/</id>
    <published>2019-09-22T06:32:21.556Z</published>
    <updated>2019-09-22T07:00:36.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="django的坑"><a href="#django的坑" class="headerlink" title="django的坑"></a>django的坑</h1><ul><li><p>modle的坑</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hBook = models.ForeignKey(<span class="string">'BookInfo'</span> , on_delete=models.CASCADE)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>搭建Django2.0+Python3+MySQL5时同步数据库时报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">django.core.exceptions.ImproperlyConfigured: mysqlclient 1.3.3 or newer is required; you have 0.7.11.None</span><br></pre></td></tr></table></figure></li></ul>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure><p>  解决办法：</p><p>  找到Python安装路劲下的Python36-32\Lib\site-packages\django\db\backends\mysql\base.py文件</p><p>  将文件中的如下代码注释</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if version &lt; (1, 3, 3):</span><br><span class="line"></span><br><span class="line">​    raise ImproperlyConfigured(&quot;mysqlclient 1.3.3 or newer is required; you have %s&quot; % Database.__version__)</span><br></pre></td></tr></table></figure><p>  <img src="//yoursite.com/2019/09/22/django的坑/Users/jlslz/AppData/Local/YNote/data/qqBD1C823FFCF91CE3AFEEE70D6EF459ED/ab5bd3b1955c47278c05c18fc40884ee/clipboard.png" alt="img"></p><p>  <img src="//yoursite.com/2019/09/22/django的坑/Users/jlslz/AppData/Local/YNote/data/qqBD1C823FFCF91CE3AFEEE70D6EF459ED/cd57f58a4a9749ca99b0371fc01c586a/clipboard.png" alt="img"></p><ul><li>path正则from django.urls import path,re_path</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re_path(&apos;(\d+)&apos;,views.detail)</span><br></pre></td></tr></table></figure><ul><li>post方法中  html《form》中添加</li></ul>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在form表单中添加</span><br><span class="line"></span><br><span class="line">​&#123;% csrf_token %&#125;  </span><br><span class="line"></span><br><span class="line">    path(&apos;booktest/&apos;,include((&apos;booktest.urls&apos;,&apos;booktest&apos;),namespace=&apos;booktest&apos;)),</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;django的坑&quot;&gt;&lt;a href=&quot;#django的坑&quot; class=&quot;headerlink&quot; title=&quot;django的坑&quot;&gt;&lt;/a&gt;django的坑&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;modle的坑&lt;/p&gt;
&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="坑" scheme="http://yoursite.com/categories/%E5%9D%91/"/>
    
    
      <category term="Django" scheme="http://yoursite.com/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>django开发流程</title>
    <link href="http://yoursite.com/2019/09/22/django%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/09/22/django开发流程/</id>
    <published>2019-09-22T06:31:05.422Z</published>
    <updated>2019-09-22T06:31:26.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="django开发流程"><a href="#django开发流程" class="headerlink" title="django开发流程"></a>django开发流程</h1><ul><li><p>step1:创建虚拟环境</p></li><li><p>step2:安装django</p></li><li><p>step3:创建项目</p></li><li><p>step4:创建应用</p></li><li><p>step5:在models.py中定义模型类</p></li><li><p>step6:定义视图</p></li><li><p>step7:配置url</p></li><li><p>step8:创建模板</p></li><li><p>django的命令：django-admin startproject 创建项目（可直接pycharm创建）</p><p>​    python manage.py startapp 创建应用</p></li></ul><h3 id="生成数据表"><a href="#生成数据表" class="headerlink" title="生成数据表"></a>生成数据表</h3><ul><li>激活模型：编辑settings.py文件，将booktest应用加入到installed_apps中</li><li><img src="//yoursite.com/2019/09/22/django开发流程/Users/jlslz/AppData/Local/YNote/data/qqBD1C823FFCF91CE3AFEEE70D6EF459ED/9ca62c01343a444cacd9bbea2f26aeac/clipboard.png" alt="img"></li></ul><p>python manage.py makemigrations 生成迁移文件：根据模型生成sql语句</p><p>​        python manage.py migrate  执行迁移：执行sql语句生成数据表</p><p>​        python manage.py runserver 运行</p><p>​        python manage.py createsuperuser 创建管理员</p><p>admin的使用admin.py</p><p>​        admin.site.register(模型类,admin类)</p><p>连接mysql 需要在<strong>init</strong></p><p>​            import pymysql</p><p>​            pymysql.install_as_MySQLdb()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;django开发流程&quot;&gt;&lt;a href=&quot;#django开发流程&quot; class=&quot;headerlink&quot; title=&quot;django开发流程&quot;&gt;&lt;/a&gt;django开发流程&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;step1:创建虚拟环境&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p
      
    
    </summary>
    
      <category term="基本操作" scheme="http://yoursite.com/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="Django" scheme="http://yoursite.com/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>jdbc连接</title>
    <link href="http://yoursite.com/2019/09/22/jdbc%E8%BF%9E%E6%8E%A5/"/>
    <id>http://yoursite.com/2019/09/22/jdbc连接/</id>
    <published>2019-09-22T06:30:09.442Z</published>
    <updated>2019-09-22T06:30:34.504Z</updated>
    
    <content type="html"><![CDATA[<h1 id="jabc连接"><a href="#jabc连接" class="headerlink" title="jabc连接"></a>jabc连接</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> jdbc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dao</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Connection conn = MySqlConn.getConn();</span><br><span class="line">    <span class="keyword">private</span> Statement stmt;</span><br><span class="line">    <span class="keyword">private</span> ResultSet rs;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Connection <span class="title">getConn</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> conn;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ResultSet <span class="title">query</span><span class="params">(String sql)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            stmt = conn.createStatement(ResultSet.TYPE_SCROLL_SENSITIVE,ResultSet.CONCUR_UPDATABLE);</span><br><span class="line">            rs = stmt.executeQuery(sql);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(<span class="string">"Data.executeQuery Error!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> rs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            rs.close();</span><br><span class="line">            stmt.close();</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(<span class="string">"关闭错误！"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> jdbc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySqlConn</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection conn;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Connection <span class="title">getConn</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">            conn = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test"</span>,<span class="string">"root"</span>,<span class="string">"1999"</span>);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            System.out.println(<span class="string">"数据库连接失败！"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> conn;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>alter table usertable auto_increment=2;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;jabc连接&quot;&gt;&lt;a href=&quot;#jabc连接&quot; class=&quot;headerlink&quot; title=&quot;jabc连接&quot;&gt;&lt;/a&gt;jabc连接&lt;/h1&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutt
      
    
    </summary>
    
      <category term="jdbc" scheme="http://yoursite.com/categories/jdbc/"/>
    
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>scrapy基本流程</title>
    <link href="http://yoursite.com/2019/09/22/scrapy%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/09/22/scrapy基本流程/</id>
    <published>2019-09-22T06:28:16.270Z</published>
    <updated>2019-09-22T06:38:04.555Z</updated>
    
    <content type="html"><![CDATA[<h1 id="scrapy初步使用"><a href="#scrapy初步使用" class="headerlink" title="scrapy初步使用"></a>scrapy初步使用</h1><ul><li><p>scrapy startproject mySpider</p></li><li><p>scrapy genspider itcast “itcast.cn”</p></li><li><p>items中添加想要的字典的键</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YangguangItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    href = scrapy.Field()</span><br><span class="line">    publish_date = scrapy.Field()</span><br><span class="line">    content_img = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure></li><li><p>piplines中可处理东西</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class YangguangPipeline(object):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        item[&quot;content&quot;] = self.process_content(item[&quot;content&quot;])</span><br><span class="line">        print(item)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def process_content(self,content):</span><br><span class="line">        content = [re.sub(r&quot;\xa0|\s&quot;,&quot;&quot;,i)  for i in content]</span><br><span class="line">        content = [i for i in content if len(i)&gt;0]</span><br><span class="line">        return content</span><br></pre></td></tr></table></figure></li></ul><ul><li>setting中加入一行,去除警告</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LEVEL =&quot;WARNING&quot;</span><br></pre></td></tr></table></figure><ul><li><p>yg.py中写</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class YgSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;yg&apos;</span><br><span class="line">    allowed_domains = [&apos;wz.sun0769.com&apos;]</span><br><span class="line">    start_urls = [&apos;http://wz.sun0769.com/index.php/question/questionType?type=4&amp;page=0&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        tr_list = response.xpath(&quot;//div[@class=&apos;greyframe&apos;]/table[2]/tr/td/table/tr&quot;)</span><br><span class="line">        for tr in tr_list:</span><br><span class="line">            item = YangguangItem()</span><br><span class="line">            item[&quot;title&quot;] = tr.xpath(&quot;./td[2]/a[@class=&apos;news14&apos;]/@title&quot;).extract_first()</span><br><span class="line">            item[&quot;href&quot;] = tr.xpath(&quot;./td[2]/a[@class=&apos;news14&apos;]/@href&quot;).extract_first()</span><br><span class="line">            item[&quot;publish_date&quot;] = tr.xpath(&quot;./td[last()]/text()&quot;).extract_first()</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                item[&quot;href&quot;],</span><br><span class="line">                callback=self.parse_detail,</span><br><span class="line">                meta=&#123;&quot;item&quot;:item&#125;</span><br><span class="line">            )</span><br><span class="line">        next_url = response.xpath(&quot;//a[text()=&apos;&gt;&apos;]/@href&quot;).extract_first()</span><br><span class="line">        if next_url is not None:</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                callback=self.parse</span><br><span class="line">            )</span><br><span class="line">    def parse_detail(self,response):</span><br><span class="line">        item = response.meta[&quot;item&quot;]</span><br><span class="line">        item[&quot;content&quot;] = response.xpath(&quot;//td[@class=&apos;txt16_3&apos;]/text()&quot;).extract()</span><br><span class="line">        item[&quot;content_img&quot;] = response.xpath(&quot;//td[@class=&apos;txt16_3&apos;]//img/@src&quot;).extract()</span><br><span class="line">        item[&quot;content_img&quot;] = [&quot;http://http://wz.sun0769.com&quot; + i for i in item[&quot;content_img&quot;]]</span><br><span class="line">        yield item</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;scrapy初步使用&quot;&gt;&lt;a href=&quot;#scrapy初步使用&quot; class=&quot;headerlink&quot; title=&quot;scrapy初步使用&quot;&gt;&lt;/a&gt;scrapy初步使用&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;scrapy startproject mySpider&lt;
      
    
    </summary>
    
      <category term="基本操作" scheme="http://yoursite.com/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>scrapy-相关</title>
    <link href="http://yoursite.com/2019/09/22/scrapy-%E7%9B%B8%E5%85%B3/"/>
    <id>http://yoursite.com/2019/09/22/scrapy-相关/</id>
    <published>2019-09-22T06:21:59.138Z</published>
    <updated>2019-09-22T07:25:00.607Z</updated>
    
    <content type="html"><![CDATA[<h3 id="logging-模块的使用"><a href="#logging-模块的使用" class="headerlink" title="logging 模块的使用"></a>logging 模块的使用</h3><ul><li>scrapy<ul><li>settings中设置LOG_LEVEL=“WARNING”</li><li>settings中设置LOG_FILE=”./a.log”  #设置日志保存的位置，设置会后终端不会显示日志内容</li><li>import logging,实例化logger的方式在任何文件中使用logger输出内容</li></ul></li><li><code>logger = logging.getLogger(__name__)       logger.warning(item)</code></li><li>普通项目中<ul><li>import logging</li><li>logging.basicConfig(…) #设置日志输出的样式，格式</li><li>实例化一个<code>logger=logging.getLogger(__name__)</code></li><li>在任何py文件中调用logger即可</li></ul></li></ul><p><img src="//yoursite.com/2019/09/22/scrapy-相关/scrapy-%E7%9B%B8%E5%85%B3%5Cday08%E6%80%BB%E7%BB%93.jpg" alt="day08总结"></p><h3 id="crawlspider的使用"><a href="#crawlspider的使用" class="headerlink" title="crawlspider的使用"></a>crawlspider的使用</h3><ul><li><p>常见爬虫 scrapy genspider -t crawl 爬虫名 allow_domain</p></li><li><p>指定start_url，对应的响应会进过rules提取url地址</p></li><li><p>完善rules，添加Rule <code>Rule(LinkExtractor(allow=r&#39;/web/site0/tab5240/info\d+\.htm&#39;), callback=&#39;parse_item&#39;),</code></p></li><li><p>注意点:</p><ul><li><p>url地址不完整，crawlspider会自动补充完整之后在请求</p></li><li><p>parse函数不能定义，他有特殊的功能需要实现</p></li><li><p>callback：连接提取器提取出来的url地址对应的响应交给他处理</p></li><li><p>follow：连接提取器提取出来的url地址对应的响应是否继续被rules来过滤</p><p><img src="//yoursite.com/2019/09/22/scrapy-相关/scrapy-%E7%9B%B8%E5%85%B3%5Cday10%E6%80%BB%E7%BB%93.jpg" alt="day10总结"><img src="//yoursite.com/2019/09/22/scrapy-相关/scrapy-%E7%9B%B8%E5%85%B3%5Cday09%E6%80%BB%E7%BB%93.jpg" alt="day09总结"></p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;logging-模块的使用&quot;&gt;&lt;a href=&quot;#logging-模块的使用&quot; class=&quot;headerlink&quot; title=&quot;logging 模块的使用&quot;&gt;&lt;/a&gt;logging 模块的使用&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;scrapy&lt;ul&gt;
&lt;li&gt;setti
      
    
    </summary>
    
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>验证码与自动化</title>
    <link href="http://yoursite.com/2019/09/22/%E9%AA%8C%E8%AF%81%E7%A0%81%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    <id>http://yoursite.com/2019/09/22/验证码与自动化/</id>
    <published>2019-09-22T06:21:03.107Z</published>
    <updated>2019-09-22T06:21:34.257Z</updated>
    
    <content type="html"><![CDATA[<h4 id="验证码的识别"><a href="#验证码的识别" class="headerlink" title="验证码的识别"></a>验证码的识别</h4><ul><li>url不变，验证码不变<ul><li>请求验证码的地址，获得相应，识别</li></ul></li><li>url不变，验证码会变<ul><li>思路：对方服务器返回验证码的时候，会和每个用户的信息和验证码进行一个对应，之后，在用户发送post请求的时候，会对比post请求中法的验证码和当前用户真正的存储在服务器端的验证码是否相同</li><li>1.实例化session</li><li>2.使用seesion请求登录页面，获取验证码的地址</li><li>3.使用session请求验证码，识别</li><li>4.使用session发送post请求’</li></ul></li><li>使用selenium登录，遇到验证码<ul><li>url不变，验证码不变，同上</li><li>url不变，验证码会变<ul><li>1.selenium请求登录页面，同时拿到验证码的地址</li><li>2.获取登录页面中driver中的cookie，交给requests模块发送验证码的请求，识别</li><li>3.输入验证码，点击登录</li></ul></li></ul></li></ul><h3 id="selenium使用的注意点"><a href="#selenium使用的注意点" class="headerlink" title="selenium使用的注意点"></a>selenium使用的注意点</h3><ul><li>获取文本和获取属性<ul><li>先定位到元素，然后调用<code>.text</code>或者<code>get_attribute</code>方法来去</li></ul></li><li>selenium获取的页面数据是浏览器中elements的内容</li><li>find_element和find_elements的区别<ul><li>find_element返回一个element，如果没有会报错</li><li>find_elements返回一个列表，没有就是空列表</li><li>在判断是否有下一页的时候，使用find_elements来根据结果的列表长度来判断</li></ul></li><li>如果页面中含有iframe、frame，需要先调用driver.switch_to.frame的方法切换到frame中才能定位元素</li><li>selenium请求第一页的时候回等待页面加载完了之后在获取数据，但是在点击翻页之后，hi直接获取数据，此时可能会报错，因为数据还没有加载出来，需要time.sleep(3)</li><li>selenium中find_element_by_class_name智能接收一个class对应的一个值，不能传入多个</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:&quot;$name&quot;,counter:&#123;$sum:2&#125;&#125;&#125;)</span><br><span class="line"></span><br><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:null,counter:&#123;$sum:1&#125;&#125;&#125;)</span><br><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:&quot;$gender&quot;,name:&#123;$push:&quot;$name&quot;&#125;&#125;&#125;)</span><br><span class="line">db.stu.aggregate(&#123;$group:&#123;_id:&quot;$gender&quot;,name:&#123;$push:&quot;$$ROOT&quot;&#125;&#125;&#125;)</span><br><span class="line">db.tv3.aggregate(</span><br><span class="line">  &#123;$group:&#123;_id:&#123;&quot;country&quot;:&quot;$country&quot;,province:&quot;$province&quot;,userid:&quot;$userid&quot;&#125;&#125;&#125;,</span><br><span class="line">  &#123;$group:&#123;_id:&#123;country:&quot;$_id.country&quot;,province:&quot;$_id.province&quot;&#125;,count:&#123;$sum:1&#125;&#125;&#125;,</span><br><span class="line">  &#123;$project:&#123;country:&quot;$_id.country&quot;,province:&quot;$_id.province&quot;,count:&quot;$count&quot;,_id:0&#125;&#125;</span><br><span class="line">  )</span><br><span class="line">db.stu.aggregate(</span><br><span class="line"></span><br><span class="line">  &#123;$match:&#123;age:&#123;$gt:20&#125;&#125;&#125;,</span><br><span class="line">  &#123;$group:&#123;_id:&quot;$gender&quot;,count:&#123;$sum:1&#125;&#125;&#125;</span><br><span class="line">  )</span><br><span class="line">db.t2.aggregate(</span><br><span class="line">  &#123;$unwind:&quot;$size&quot;&#125;</span><br><span class="line">  )</span><br><span class="line">db.t3.aggregate(</span><br><span class="line">  &#123;$unwind:&quot;$tags&quot;&#125;,</span><br><span class="line">  &#123;$group:&#123;_id:null,count:&#123;$sum:1&#125;&#125;&#125;</span><br><span class="line">  )</span><br><span class="line">db.t3.aggregate(</span><br><span class="line">  &#123;$unwind:&#123;path:&quot;$size&quot;,preserveNullAndEmptyArrays:true&#125;&#125;</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;验证码的识别&quot;&gt;&lt;a href=&quot;#验证码的识别&quot; class=&quot;headerlink&quot; title=&quot;验证码的识别&quot;&gt;&lt;/a&gt;验证码的识别&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;url不变，验证码不变&lt;ul&gt;
&lt;li&gt;请求验证码的地址，获得相应，识别&lt;/li&gt;
&lt;/ul&gt;

      
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫基本操作</title>
    <link href="http://yoursite.com/2019/09/22/%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2019/09/22/爬虫基本操作/</id>
    <published>2019-09-22T06:20:10.684Z</published>
    <updated>2019-09-22T06:32:07.689Z</updated>
    
    <content type="html"><![CDATA[<h3 id="xpath的包含"><a href="#xpath的包含" class="headerlink" title="xpath的包含"></a>xpath的包含</h3><ul><li><code>//div[contains(@class,&#39;i&#39;)]</code></li></ul><h3 id="实现爬虫的套路"><a href="#实现爬虫的套路" class="headerlink" title="实现爬虫的套路"></a>实现爬虫的套路</h3><ul><li>准备url<ul><li>准备start_url<ul><li>url地址规律不明显，总数不确定</li><li>通过代码提取下一页的url<ul><li>xpath</li><li>寻找url地址，部分参数在当前的响应中（比如，当前页码数和总的页码数在当前的响应中）</li></ul></li></ul></li><li>准备url_list<ul><li>页码总数明确</li><li>url地址规律明显</li></ul></li></ul></li><li>发送请求，获取响应<ul><li>添加随机的User-Agent,反反爬虫</li><li>添加随机的代理ip，反反爬虫</li><li>在对方判断出我们是爬虫之后，应该添加更多的headers字段，包括cookie</li><li>cookie的处理可以使用session来解决</li><li>准备一堆能用的cookie，组成cookie池<ul><li>如果不登录<ul><li>准备刚开始能够成功请求对方网站的cookie，即接收对方网站设置在response的cookie</li><li>下一次请求的时候，使用之前的列表中的cookie来请求</li></ul></li><li>如果登录<ul><li>准备多个账号</li><li>使用程序获取每个账号的cookie</li><li>之后请求登录之后才能访问的网站随机的选择cookie</li></ul></li></ul></li></ul></li><li>提取数据<ul><li>确定数据的位置<ul><li>如果数据在当前的url地址中<ul><li>提取的是列表页的数据<ul><li>直接请求列表页的url地址，不用进入详情页</li></ul></li><li>提取的是详情页的数据<ul><li><ol><li>确定url</li></ol></li><li><ol start="2"><li>发送请求</li></ol></li><li><ol start="3"><li>提取数据</li></ol></li><li><ol start="4"><li>返回</li></ol></li></ul></li></ul></li><li>如果数据不在当前的url地址中<ul><li>在其他的响应中，寻找数据的位置<ul><li><ol><li>从network中从上往下找</li></ol></li><li><ol start="2"><li>使用chrome中的过滤条件，选择出了js,css,img之外的按钮</li></ol></li><li><ol start="3"><li>使用chrome的search all file，搜索数字和英文</li></ol></li></ul></li></ul></li></ul></li><li>数据的提取<ul><li>xpath,从html中提取整块的数据，先分组，之后每一组再提取</li><li>re，提取max_time,price,html中的json字符串</li><li>json</li></ul></li></ul></li></ul><ul><li>保存<ul><li>保存在本地，text,json,csv</li><li>保存在数据库</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;xpath的包含&quot;&gt;&lt;a href=&quot;#xpath的包含&quot; class=&quot;headerlink&quot; title=&quot;xpath的包含&quot;&gt;&lt;/a&gt;xpath的包含&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;//div[contains(@class,&amp;#39;i&amp;#39;)
      
    
    </summary>
    
      <category term="基本操作" scheme="http://yoursite.com/categories/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>正则与XPATH</title>
    <link href="http://yoursite.com/2019/09/22/%E6%AD%A3%E5%88%99%E4%B8%8EXPATH/"/>
    <id>http://yoursite.com/2019/09/22/正则与XPATH/</id>
    <published>2019-09-22T06:19:29.552Z</published>
    <updated>2019-09-22T06:19:49.002Z</updated>
    
    <content type="html"><![CDATA[<h3 id="正则使用的注意点"><a href="#正则使用的注意点" class="headerlink" title="正则使用的注意点"></a>正则使用的注意点</h3><ul><li><code>re.findall(&quot;a(.*?)b&quot;,&quot;str&quot;)</code>,能够返回括号中的内容,括号前后的内容起到定位和过滤的效果</li><li>原始字符串r，待匹配字符串中有反斜杠的时候，使用r能够忽视反斜杠带来的转义的效果</li><li>点号默认情况匹配不到<code>\n</code></li><li><code>\s</code>能够匹配空白字符，不仅仅包含空格，还有<code>\t|\r\n</code></li></ul><h3 id="xpath学习重点"><a href="#xpath学习重点" class="headerlink" title="xpath学习重点"></a>xpath学习重点</h3><ul><li>使用xpath helper或者是chrome中的copy xpath都是从element中提取的数据，但是爬虫获取的是url对应的响应，往往和elements不一样</li><li>获取文本<ul><li><code>a/text()</code> 获取a下的文本</li><li><code>a//text()</code> 获取a下的所有标签的文本</li><li><code>//a[text()=&#39;下一页&#39;]</code> 选择文本为下一页三个字的a标签</li></ul></li><li><code>@符号</code><ul><li><code>a/@href</code></li><li><code>//ul[@id=&quot;detail-list&quot;]</code></li></ul></li><li><code>//</code><ul><li>在xpath最前面表示从当前html中任意位置开始选择</li><li><code>li//a</code> 表示的是li下任何一个标签</li></ul></li></ul><h3 id="lxml使用注意点"><a href="#lxml使用注意点" class="headerlink" title="lxml使用注意点"></a>lxml使用注意点</h3><ul><li>lxml能够修正HTML代码，但是可能会改错了<ul><li>使用etree.tostring观察修改之后的html的样子，根据修改之后的html字符串写xpath</li></ul></li><li>lxml 能够接受bytes和str的字符串</li><li>提取页面数据的思路<ul><li>先分组，渠道一个包含分组标签的列表</li><li>遍历，取其中每一组进行数据的提取，不会造成数据的对应错乱</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;正则使用的注意点&quot;&gt;&lt;a href=&quot;#正则使用的注意点&quot; class=&quot;headerlink&quot; title=&quot;正则使用的注意点&quot;&gt;&lt;/a&gt;正则使用的注意点&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;re.findall(&amp;quot;a(.*?)b&amp;quot;,&amp;qu
      
    
    </summary>
    
      <category term="正则" scheme="http://yoursite.com/categories/%E6%AD%A3%E5%88%99/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>寻找post地址</title>
    <link href="http://yoursite.com/2019/09/22/%E5%AF%BB%E6%89%BEpost%E5%9C%B0%E5%9D%80/"/>
    <id>http://yoursite.com/2019/09/22/寻找post地址/</id>
    <published>2019-09-22T06:18:47.260Z</published>
    <updated>2019-09-22T06:19:07.651Z</updated>
    
    <content type="html"><![CDATA[<h3 id="寻找登录的post地址"><a href="#寻找登录的post地址" class="headerlink" title="寻找登录的post地址"></a>寻找登录的post地址</h3><ul><li>在form表单中寻找action对应的url地址<ul><li>post的数据是input标签中name的值作为键，真正的用户名密码作为值的字典，post的url地址就是action对应的url地址</li></ul></li><li>抓包，寻找登录的url地址<ul><li>勾选perserve log按钮，防止页面跳转找不到url</li><li>寻找post数据，确定参数<ul><li>参数不会变，直接用，比如密码不是动态加密的时候</li><li>参数会变<ul><li>参数在当前的响应中</li><li>通过js生成</li></ul></li></ul></li></ul></li></ul><h3 id="定位想要的js"><a href="#定位想要的js" class="headerlink" title="定位想要的js"></a>定位想要的js</h3><ul><li>选择会触发js时间的按钮，点击event listener，找到js的位置</li><li>通过chrome中的search all file来搜索url中关键字</li><li>添加断点的方式来查看js的操作，通过python来进行同样的操作</li></ul><h3 id="安装第三方模块"><a href="#安装第三方模块" class="headerlink" title="安装第三方模块"></a>安装第三方模块</h3><ul><li>pip install retrying</li><li>下载源码解码，进入解压后的目录，<code>python setup.py install</code></li><li><code>***.whl</code> 安装方法 <code>pip install ***.whl</code></li></ul><h3 id="json使用注意点"><a href="#json使用注意点" class="headerlink" title="json使用注意点"></a>json使用注意点</h3><ul><li>json中的字符串都是双引号引起来的<ul><li>如果不是双引号<ul><li>eval：能实现简单的字符串和python类型的转化</li><li>replace：把单引号替换为双引号</li></ul></li></ul></li><li>往一个文件中写入多个json串，不再是一个json串，不能直接读取<ul><li>一行写一个json串，按照行来读取</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;寻找登录的post地址&quot;&gt;&lt;a href=&quot;#寻找登录的post地址&quot; class=&quot;headerlink&quot; title=&quot;寻找登录的post地址&quot;&gt;&lt;/a&gt;寻找登录的post地址&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在form表单中寻找action对应的url地址&lt;ul&gt;
      
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫概念</title>
    <link href="http://yoursite.com/2019/09/22/%E7%88%AC%E8%99%AB%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2019/09/22/爬虫概念/</id>
    <published>2019-09-22T06:06:23.680Z</published>
    <updated>2019-09-22T06:07:41.474Z</updated>
    
    <content type="html"><![CDATA[<h3 id="爬虫的概念"><a href="#爬虫的概念" class="headerlink" title="爬虫的概念"></a>爬虫的概念</h3><ul><li>爬虫是模拟浏览器发送请求，获取响应</li></ul><h3 id="爬虫的流程"><a href="#爬虫的流程" class="headerlink" title="爬虫的流程"></a>爬虫的流程</h3><ul><li>url—&gt;发送请求，获取响应—&gt;提取数据—》保存</li><li>发送请求，获取响应—&gt;提取url</li></ul><h4 id="爬虫要根据当前url地址对应的响应为准-，当前url地址的elements的内容和url的响应不一样"><a href="#爬虫要根据当前url地址对应的响应为准-，当前url地址的elements的内容和url的响应不一样" class="headerlink" title="爬虫要根据当前url地址对应的响应为准 ，当前url地址的elements的内容和url的响应不一样"></a>爬虫要根据当前url地址对应的响应为准 ，当前url地址的elements的内容和url的响应不一样</h4><h3 id="页面上的数据在哪里"><a href="#页面上的数据在哪里" class="headerlink" title="页面上的数据在哪里"></a>页面上的数据在哪里</h3><ul><li>当前url地址对应的响应中</li><li>其他的url地址对应的响应中<ul><li>比如ajax请求中</li></ul></li><li>js生成的<ul><li>部分数据在响应中</li><li>全部通过js生成</li></ul></li></ul><h3 id="requests中解决编解码的方法"><a href="#requests中解决编解码的方法" class="headerlink" title="requests中解决编解码的方法"></a>requests中解决编解码的方法</h3><ul><li>response.content.decode()</li><li>response.content.decode(“gbk”)</li><li>response.text</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;爬虫的概念&quot;&gt;&lt;a href=&quot;#爬虫的概念&quot; class=&quot;headerlink&quot; title=&quot;爬虫的概念&quot;&gt;&lt;/a&gt;爬虫的概念&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;爬虫是模拟浏览器发送请求，获取响应&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;爬虫的流程&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="概念" scheme="http://yoursite.com/categories/%E6%A6%82%E5%BF%B5/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>枚举法</title>
    <link href="http://yoursite.com/2019/07/27/test/"/>
    <id>http://yoursite.com/2019/07/27/test/</id>
    <published>2019-07-27T14:38:37.165Z</published>
    <updated>2019-07-01T12:50:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="枚举法"><a href="#枚举法" class="headerlink" title="枚举法"></a>枚举法</h1><h2 id="输入-5-5-5-5-5-5"><a href="#输入-5-5-5-5-5-5" class="headerlink" title="输入 5 5 5 5 5 = 5"></a>输入 5 5 5 5 5 = 5</h2><h2 id="判断中间填什么，可以左右两边相等"><a href="#判断中间填什么，可以左右两边相等" class="headerlink" title="判断中间填什么，可以左右两边相等"></a>判断中间填什么，可以左右两边相等</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> j,i[<span class="number">5</span>];</span><br><span class="line">    <span class="keyword">int</span> sign;</span><br><span class="line">    <span class="keyword">int</span> result;</span><br><span class="line">    <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> num[<span class="number">6</span>];</span><br><span class="line">    <span class="keyword">float</span> left,right;</span><br><span class="line">    <span class="keyword">char</span> oper[<span class="number">5</span>]=&#123;<span class="string">' '</span>,<span class="string">'+'</span>,<span class="string">'-'</span>,<span class="string">'*'</span>,<span class="string">'/'</span>&#125;;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"输入5个数"</span>);</span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">1</span>; j &lt;=<span class="number">5</span>; ++j) &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;num[j]);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"请输入结果"</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;result);</span><br><span class="line">    <span class="keyword">for</span> ( i[<span class="number">1</span>] = <span class="number">1</span>; i[<span class="number">1</span>] &lt;=<span class="number">4</span> ; i[<span class="number">1</span>]++) &#123;</span><br><span class="line">        <span class="keyword">if</span>((i[<span class="number">1</span>]&lt;<span class="number">4</span>)||(num2!=<span class="number">0</span>))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (i[<span class="number">2</span>] = <span class="number">1</span>;  i[<span class="number">2</span>]&lt;=<span class="number">4</span> ; i[<span class="number">2</span>]++) &#123;</span><br><span class="line">                <span class="keyword">if</span> ((i[<span class="number">2</span>]&lt;<span class="number">4</span>)||(num[<span class="number">3</span>])!=<span class="number">0</span>)&#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> (i[<span class="number">3</span>] = <span class="number">1</span>; i[<span class="number">3</span>] &lt; <span class="number">4</span>; i[<span class="number">3</span>]++) &#123;</span><br><span class="line">                        <span class="keyword">if</span>((i[<span class="number">3</span>]&lt;<span class="number">4</span>)||num[<span class="number">4</span>]!=<span class="number">0</span>)&#123;</span><br><span class="line">                            <span class="keyword">for</span> (i[<span class="number">4</span>] =<span class="number">1</span>;  i[<span class="number">4</span>]&lt;=<span class="number">4</span> ; i[<span class="number">4</span>]++) &#123;</span><br><span class="line">                                <span class="keyword">if</span>((i[<span class="number">4</span>]&lt;<span class="number">4</span>)||num[<span class="number">5</span>]!=<span class="number">0</span>)&#123;</span><br><span class="line">                                    left =<span class="number">0</span>;</span><br><span class="line">                                    right=num[<span class="number">1</span>];</span><br><span class="line">                                    sign=<span class="number">1</span>;</span><br><span class="line">                                    <span class="keyword">for</span> (j =<span class="number">1</span>;  j&lt;=<span class="number">4</span> ; j++) &#123;</span><br><span class="line">                                        <span class="keyword">switch</span>(oper[i[j]])</span><br><span class="line">                                        &#123;</span><br><span class="line">                                            <span class="keyword">case</span> <span class="string">'+'</span>:</span><br><span class="line">                                                left=left+sign*right;</span><br><span class="line">                                                sign=<span class="number">1</span>;</span><br><span class="line">                                                right=num[j+<span class="number">1</span>];</span><br><span class="line">                                                <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                            <span class="keyword">case</span> <span class="string">'-'</span>:</span><br><span class="line">                                                left=left+sign*right;</span><br><span class="line">                                                sign=<span class="number">-1</span>;</span><br><span class="line">                                                right=num[j+<span class="number">1</span>];</span><br><span class="line">                                                <span class="keyword">break</span>;</span><br><span class="line">                                            <span class="keyword">case</span> <span class="string">'*'</span>:</span><br><span class="line">                                                right=right*num[j+<span class="number">1</span>];</span><br><span class="line">                                                <span class="keyword">break</span>;</span><br><span class="line">                                            <span class="keyword">case</span> <span class="string">'/'</span>:</span><br><span class="line">                                                right =right/num[j+<span class="number">1</span>];</span><br><span class="line">                                                <span class="keyword">break</span>;</span><br><span class="line">                                        &#125;</span><br><span class="line"></span><br><span class="line">                                    &#125;</span><br><span class="line">                                    <span class="keyword">if</span>(left+sing*right == result)</span><br><span class="line"></span><br><span class="line">                                    &#123;</span><br><span class="line">                                        count++;</span><br><span class="line">                                        <span class="built_in">printf</span>(<span class="string">"%3d:"</span>,count);</span><br><span class="line">                                        <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt;<span class="number">4</span> ; j++) &#123;</span><br><span class="line">                                            <span class="built_in">printf</span>(<span class="string">"%d%c"</span>,num[j],oper[i[j]]);</span><br><span class="line">                                            <span class="built_in">printf</span>(<span class="string">"%d=%d\n"</span>,num[<span class="number">5</span>],result);</span><br><span class="line"></span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;                                &#125;</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;枚举法&quot;&gt;&lt;a href=&quot;#枚举法&quot; class=&quot;headerlink&quot; title=&quot;枚举法&quot;&gt;&lt;/a&gt;枚举法&lt;/h1&gt;&lt;h2 id=&quot;输入-5-5-5-5-5-5&quot;&gt;&lt;a href=&quot;#输入-5-5-5-5-5-5&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="C" scheme="http://yoursite.com/categories/C/"/>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
